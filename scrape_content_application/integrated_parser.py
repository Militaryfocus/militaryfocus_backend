"""
–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –ø–æ–ª–Ω–æ–π —Ü–µ–ø–æ—á–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏:
–ü–∞—Ä—Å–∏–Ω–≥ -> –ò–ò –æ–±—Ä–∞–±–æ—Ç–∫–∞ -> –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ -> –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
"""
import asyncio
import aiohttp
import time
import logging
import os
import sys
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append('/workspace')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'war_site.settings')

import django
django.setup()

from scrape_content_application.models import ContentSource, ArticleContent, ArticleTag, ArticleTagRelation
from scrape_content_application.ai_content_processor import get_ai_processor, ProcessedContent
from scrape_content_application.content_analyzer import get_content_analyzer
from scrape_content_application.advanced_parser import AdvancedContentScraper, BaseParser
from scrape_content_application.utils import log_parsing_activity
from asgiref.sync import sync_to_async

logger = logging.getLogger(__name__)


@dataclass
class ProcessingResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏"""
    success: bool
    article_id: Optional[int] = None
    original_title: str = ""
    processed_title: str = ""
    original_content: str = ""
    processed_content: str = ""
    quality_score: float = 0.0
    uniqueness_score: float = 0.0
    categories: List[str] = None
    tags: List[str] = None
    is_duplicate: bool = False
    processing_time: float = 0.0
    error_message: str = ""
    
    def __post_init__(self):
        if self.categories is None:
            self.categories = []
        if self.tags is None:
            self.tags = []


class IntegratedContentProcessor:
    """
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–ª–Ω–æ–π —Ü–µ–ø–æ—á–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    
    def __init__(self):
        self.ai_processor = get_ai_processor()
        self.content_analyzer = get_content_analyzer()
        self.session = None
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        self.min_quality_score = 60.0  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        self.min_uniqueness_score = 70.0  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_processed': 0,
            'successful_saves': 0,
            'duplicates_found': 0,
            'low_quality_rejected': 0,
            'ai_processing_failures': 0,
            'total_processing_time': 0.0
        }
    
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=60),
            connector=aiohttp.TCPConnector(limit=10)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥"""
        if self.session:
            await self.session.close()
    
    async def download_and_save_image(self, image_url: str, article_id: int) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            async with self.session.get(image_url) as response:
                if response.status == 200:
                    content = await response.read()
                    
                    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                    media_dir = '/workspace/media/articles'
                    os.makedirs(media_dir, exist_ok=True)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
                    file_extension = image_url.split('.')[-1].lower()
                    if file_extension not in ['jpg', 'jpeg', 'png', 'webp', 'gif']:
                        file_extension = 'jpg'
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
                    timestamp = int(time.time())
                    filename = f"article_{article_id}_{timestamp}.{file_extension}"
                    filepath = os.path.join(media_dir, filename)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
                    with open(filepath, 'wb') as f:
                        f.write(content)
                    
                    logger.info(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
                    return f"articles/{filename}"
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_url}: {e}")
        
        return None
    
    async def create_tags_for_article(self, article: ArticleContent, tags: List[str]) -> int:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –ø—Ä–∏–≤—è–∑–∫–∞ —Ç–µ–≥–æ–≤ –∫ —Å—Ç–∞—Ç—å–µ"""
        created_tags = 0
        
        for tag_name in tags[:8]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 8 —Ç–µ–≥–æ–≤
            try:
                # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –ø–æ–ª—É—á–∞–µ–º —Ç–µ–≥
                tag_slug = tag_name.lower().replace(' ', '-').replace('_', '-')
                tag_slug = ''.join(c for c in tag_slug if c.isalnum() or c == '-')
                
                tag, created = await sync_to_async(ArticleTag.objects.get_or_create)(
                    name=tag_name,
                    defaults={
                        'slug': tag_slug,
                        'description': f"–°—Ç–∞—Ç—å–∏ –ø–æ —Ç–µ–º–µ: {tag_name}"
                    }
                )
                
                # –°–æ–∑–¥–∞–µ–º —Å–≤—è–∑—å —Å—Ç–∞—Ç—å—è-—Ç–µ–≥
                relation, relation_created = await sync_to_async(ArticleTagRelation.objects.get_or_create)(
                    article=article,
                    tag=tag
                )
                
                if relation_created:
                    created_tags += 1
                    logger.debug(f"–°–æ–∑–¥–∞–Ω–∞ —Å–≤—è–∑—å —Å—Ç–∞—Ç—å—è-—Ç–µ–≥: {article.article_title[:30]}... -> {tag_name}")
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ–≥–∞ '{tag_name}': {e}")
        
        return created_tags
    
    async def process_article_content(self, raw_article_data: Dict, source: ContentSource) -> ProcessingResult:
        """
        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: –ò–ò -> –ê–Ω–∞–ª–∏–∑ -> –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        """
        start_time = time.time()
        self.stats['total_processed'] += 1
        
        result = ProcessingResult(
            success=False,
            original_title=raw_article_data.get('title', ''),
            original_content=raw_article_data.get('content', '')
        )
        
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å—Ç–∞—Ç—å–∏: {result.original_title[:50]}...")
            
            # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
            logger.debug("–®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã")
            duplicate_analysis = await self.content_analyzer.analyze_content(
                result.original_title,
                result.original_content,
                raw_article_data.get('link', '')
            )
            
            if duplicate_analysis['duplicate_check']['is_duplicate']:
                result.is_duplicate = True
                result.error_message = f"–ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç (—Å—Ö–æ–∂–µ—Å—Ç—å: {duplicate_analysis['duplicate_check']['similarity_score']:.2f})"
                self.stats['duplicates_found'] += 1
                logger.info(f"–°—Ç–∞—Ç—å—è –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç: {result.original_title[:50]}...")
                return result
            
            # –®–∞–≥ 2: –ò–ò –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            logger.debug("–®–∞–≥ 2: –ò–ò –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
            try:
                processed_content: ProcessedContent = await self.ai_processor.process_content(
                    result.original_title,
                    result.original_content
                )
                
                result.processed_title = processed_content.processed_title
                result.processed_content = processed_content.processed_content
                result.quality_score = processed_content.quality.overall_score
                result.uniqueness_score = processed_content.quality.uniqueness_score
                result.categories = [cat.category for cat in duplicate_analysis.get('categories', [])]
                result.tags = processed_content.tags
                
                logger.info(f"–ò–ò –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ö–∞—á–µ—Å—Ç–≤–æ: {result.quality_score:.1f}, –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {result.uniqueness_score:.1f}")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ò–ò –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                self.stats['ai_processing_failures'] += 1
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –µ—Å–ª–∏ –ò–ò –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
                result.processed_title = result.original_title
                result.processed_content = result.original_content
                result.quality_score = 50.0  # –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞
                result.uniqueness_score = 50.0
                result.categories = [cat['category'] for cat in duplicate_analysis.get('categories', [])]
                result.tags = ['–í–æ–µ–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏', '–û–±–æ—Ä–æ–Ω–∞']  # –ë–∞–∑–æ–≤—ã–µ —Ç–µ–≥–∏
            
            # –®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            logger.debug("–®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞")
            if result.quality_score < self.min_quality_score:
                result.error_message = f"–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {result.quality_score:.1f} < {self.min_quality_score}"
                self.stats['low_quality_rejected'] += 1
                logger.warning(f"–°—Ç–∞—Ç—å—è –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É: {result.original_title[:50]}...")
                return result
            
            if result.uniqueness_score < self.min_uniqueness_score:
                result.error_message = f"–ù–∏–∑–∫–∞—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {result.uniqueness_score:.1f} < {self.min_uniqueness_score}"
                self.stats['low_quality_rejected'] += 1
                logger.warning(f"–°—Ç–∞—Ç—å—è –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏: {result.original_title[:50]}...")
                return result
            
            # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            logger.debug("–®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            import hashlib
            content_hash = hashlib.md5(result.processed_content.encode('utf-8')).hexdigest()
            
            article = ArticleContent(
                article_title=result.processed_title,
                article_content=result.processed_content,
                article_summary=processed_content.summary if 'processed_content' in locals() else result.processed_content[:200] + "...",
                article_link=raw_article_data.get('link', ''),
                source=source,
                status='published',
                content_type='news',
                published_at=raw_article_data.get('published_at'),
                word_count=len(result.processed_content.split()),
                meta_keywords=', '.join(result.tags[:10]) if result.tags else '',
                meta_description=(processed_content.summary if 'processed_content' in locals() else result.processed_content[:160]),
                is_ai_processed=True,
                is_featured=result.quality_score > 85.0,  # –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ
                content_hash=content_hash,
                quality_score=result.quality_score,
                uniqueness_score=result.uniqueness_score
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—å—é
            await sync_to_async(article.save)()
            result.article_id = article.id
            
            logger.info(f"–°—Ç–∞—Ç—å—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å ID: {article.id}")
            
            # –®–∞–≥ 5: –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if raw_article_data.get('image_url'):
                logger.debug("–®–∞–≥ 5: –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                image_path = await self.download_and_save_image(raw_article_data['image_url'], article.id)
                if image_path:
                    article.article_image = image_path
                    await sync_to_async(article.save)(update_fields=['article_image'])
                    logger.debug(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–∏–≤—è–∑–∞–Ω–æ –∫ —Å—Ç–∞—Ç—å–µ: {image_path}")
            
            # –®–∞–≥ 6: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–≥–æ–≤
            if result.tags:
                logger.debug("–®–∞–≥ 6: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–≥–æ–≤")
                tags_created = await self.create_tags_for_article(article, result.tags)
                logger.debug(f"–°–æ–∑–¥–∞–Ω–æ —Ç–µ–≥–æ–≤: {tags_created}")
            
            # –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            result.success = True
            self.stats['successful_saves'] += 1
            
            logger.info(f"‚úÖ –°—Ç–∞—Ç—å—è —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {result.processed_title[:50]}...")
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏: {e}")
            result.error_message = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
        
        finally:
            result.processing_time = time.time() - start_time
            self.stats['total_processing_time'] += result.processing_time
        
        return result
    
    async def process_source_with_full_pipeline(self, source: ContentSource) -> Dict:
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
        """
        start_time = time.time()
        articles_found = 0
        articles_saved = 0
        articles_rejected = 0
        processing_results = []
        error_message = ""
        
        try:
            logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source.name}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            scraper = AdvancedContentScraper()
            scraper.session = self.session
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            parser = scraper.get_parser_for_source(source)
            if not parser:
                raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø–∞—Ä—Å–µ—Ä")
            
            parser.session = self.session
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
            article_links = await parser.parse_article_list(source.source_link)
            articles_found = len(article_links)
            
            if not article_links:
                logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source.name}")
                return {
                    'status': 'success',
                    'articles_found': 0,
                    'articles_saved': 0,
                    'articles_rejected': 0,
                    'execution_time': time.time() - start_time,
                    'processing_results': []
                }
            
            logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {articles_found} —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç–∞—Ç—å—é —á–µ—Ä–µ–∑ –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
            for i, link in enumerate(article_links, 1):
                try:
                    logger.info(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å—é {i}/{len(article_links)}")
                    
                    # –ü–∞—Ä—Å–∏–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏
                    raw_article_data = await parser.parse_article_content(link)
                    if not raw_article_data:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç–∞—Ç—å—é: {link}")
                        continue
                    
                    # –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ –ò–ò –∏ –∞–Ω–∞–ª–∏–∑
                    processing_result = await self.process_article_content(raw_article_data, source)
                    processing_results.append(processing_result)
                    
                    if processing_result.success:
                        articles_saved += 1
                        logger.info(f"‚úÖ –°—Ç–∞—Ç—å—è {i} —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (ID: {processing_result.article_id})")
                    else:
                        articles_rejected += 1
                        logger.warning(f"‚ùå –°—Ç–∞—Ç—å—è {i} –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞: {processing_result.error_message}")
                    
                    # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å—Ç–∞—Ç–µ–π
                    await asyncio.sleep(3)
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏ {link}: {e}")
                    articles_rejected += 1
                    continue
            
            execution_time = time.time() - start_time
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –±–∞–∑—É
            await sync_to_async(log_parsing_activity)(
                source=source,
                status='success',
                articles_found=articles_found,
                articles_saved=articles_saved,
                execution_time=execution_time
            )
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            avg_quality = sum(r.quality_score for r in processing_results if r.success) / max(1, articles_saved)
            avg_uniqueness = sum(r.uniqueness_score for r in processing_results if r.success) / max(1, articles_saved)
            avg_processing_time = sum(r.processing_time for r in processing_results) / max(1, len(processing_results))
            
            logger.info(f"üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source.name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
            logger.info(f"   üìä –ù–∞–π–¥–µ–Ω–æ: {articles_found}, –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {articles_saved}, –û—Ç–∫–ª–æ–Ω–µ–Ω–æ: {articles_rejected}")
            logger.info(f"   ‚≠ê –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {avg_quality:.1f}, –°—Ä–µ–¥–Ω—è—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {avg_uniqueness:.1f}")
            logger.info(f"   ‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {execution_time:.1f}—Å (—Å—Ä–µ–¥–Ω–µ–µ –Ω–∞ —Å—Ç–∞—Ç—å—é: {avg_processing_time:.1f}—Å)")
            
            return {
                'status': 'success',
                'articles_found': articles_found,
                'articles_saved': articles_saved,
                'articles_rejected': articles_rejected,
                'execution_time': execution_time,
                'processing_results': processing_results,
                'quality_metrics': {
                    'avg_quality_score': avg_quality,
                    'avg_uniqueness_score': avg_uniqueness,
                    'avg_processing_time': avg_processing_time
                }
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_message = str(e)
            
            logger.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source.name}: {e}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
            await sync_to_async(log_parsing_activity)(
                source=source,
                status='error',
                articles_found=articles_found,
                articles_saved=articles_saved,
                error_message=error_message,
                execution_time=execution_time
            )
            
            return {
                'status': 'error',
                'error': error_message,
                'articles_found': articles_found,
                'articles_saved': articles_saved,
                'articles_rejected': articles_rejected,
                'execution_time': execution_time,
                'processing_results': processing_results
            }
    
    async def run_full_pipeline_for_all_sources(self) -> Dict:
        """
        –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
        logger.info("üåü –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        sources = await sync_to_async(list)(
            ContentSource.objects.filter(
                is_enabled=True,
                status='active'
            )
        )
        
        if not sources:
            logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return {
                'status': 'warning',
                'message': '–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤',
                'results': []
            }
        
        logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(sources)} –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
        results = []
        total_found = 0
        total_saved = 0
        total_rejected = 0
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
        for i, source in enumerate(sources, 1):
            logger.info(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ {i}/{len(sources)}: {source.name}")
            
            result = await self.process_source_with_full_pipeline(source)
            results.append({
                'source_name': source.name,
                'source_id': source.id,
                'result': result
            })
            
            total_found += result.get('articles_found', 0)
            total_saved += result.get('articles_saved', 0)
            total_rejected += result.get('articles_rejected', 0)
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
            await asyncio.sleep(5)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info("üèÅ –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω!")
        logger.info(f"üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        logger.info(f"   üìÑ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {total_found}")
        logger.info(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {total_saved}")
        logger.info(f"   ‚ùå –û—Ç–∫–ª–æ–Ω–µ–Ω–æ: {total_rejected}")
        logger.info(f"   üìà –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {(total_saved / max(1, total_found)) * 100:.1f}%")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        logger.info(f"üîß –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        for key, value in self.stats.items():
            if key == 'total_processing_time':
                logger.info(f"   {key}: {value:.1f}—Å")
            else:
                logger.info(f"   {key}: {value}")
        
        return {
            'status': 'success',
            'total_sources': len(sources),
            'total_found': total_found,
            'total_saved': total_saved,
            'total_rejected': total_rejected,
            'success_rate': (total_saved / max(1, total_found)) * 100,
            'processing_stats': self.stats,
            'results': results
        }
    
    def get_processing_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        return {
            'stats': self.stats.copy(),
            'avg_processing_time': (
                self.stats['total_processing_time'] / max(1, self.stats['total_processed'])
            ),
            'success_rate': (
                self.stats['successful_saves'] / max(1, self.stats['total_processed'])
            ) * 100,
            'rejection_rate': (
                (self.stats['duplicates_found'] + self.stats['low_quality_rejected']) / 
                max(1, self.stats['total_processed'])
            ) * 100
        }


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞"""
    async with IntegratedContentProcessor() as processor:
        result = await processor.run_full_pipeline_for_all_sources()
        
        print("\n" + "="*80)
        print("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–û–ì–û –ü–ê–†–°–ï–†–ê")
        print("="*80)
        print(f"–°—Ç–∞—Ç—É—Å: {result['status']}")
        print(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result.get('total_sources', 0)}")
        print(f"–°—Ç–∞—Ç–µ–π –Ω–∞–π–¥–µ–Ω–æ: {result.get('total_found', 0)}")
        print(f"–°—Ç–∞—Ç–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {result.get('total_saved', 0)}")
        print(f"–°—Ç–∞—Ç–µ–π –æ—Ç–∫–ª–æ–Ω–µ–Ω–æ: {result.get('total_rejected', 0)}")
        print(f"–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {result.get('success_rate', 0):.1f}%")
        print("="*80)
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        for source_result in result.get('results', []):
            source_name = source_result['source_name']
            source_data = source_result['result']
            print(f"\nüì∞ {source_name}:")
            print(f"   –ù–∞–π–¥–µ–Ω–æ: {source_data.get('articles_found', 0)}")
            print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {source_data.get('articles_saved', 0)}")
            print(f"   –û—Ç–∫–ª–æ–Ω–µ–Ω–æ: {source_data.get('articles_rejected', 0)}")
            if 'quality_metrics' in source_data:
                qm = source_data['quality_metrics']
                print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {qm.get('avg_quality_score', 0):.1f}")
                print(f"   –°—Ä–µ–¥–Ω—è—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {qm.get('avg_uniqueness_score', 0):.1f}")


if __name__ == "__main__":
    asyncio.run(main())